{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4297e2",
   "metadata": {},
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438a2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch 0, Loss 3.2169\n",
      "Epoch 1, Loss 1.9979\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchgeo.datasets import VHR10\n",
    "import random\n",
    "\n",
    "# ----- Step 1: Load dataset -----\n",
    "def preprocess(sample):\n",
    "    sample[\"image\"] = sample[\"image\"].float() / 255.0\n",
    "    return sample\n",
    "\n",
    "ds = VHR10(\n",
    "    root=\"data/VHR10/\",\n",
    "    split=\"positive\",\n",
    "    transforms=preprocess,\n",
    "    download=True,\n",
    "    checksum=True,\n",
    ")\n",
    "\n",
    "# ----- Step 2: Split base vs novel classes -----\n",
    "# NWPU VHR-10: 10 classes (1–10). Giả sử chọn 3 novel classes: airplane=1, baseball diamond=4, tennis court=5\n",
    "novel_classes = [1, 4, 5]\n",
    "base_classes = [c for c in range(1, 11) if c not in novel_classes]\n",
    "\n",
    "base_indices, novel_indices = [], []\n",
    "for i in range(len(ds)):\n",
    "    labels = ds[i][\"label\"]\n",
    "    if any(l in novel_classes for l in labels):\n",
    "        novel_indices.append(i)\n",
    "    else:\n",
    "        base_indices.append(i)\n",
    "\n",
    "base_ds = Subset(ds, base_indices)\n",
    "novel_ds = Subset(ds, novel_indices)\n",
    "\n",
    "# Few-shot: chọn K=5 ảnh cho mỗi novel class\n",
    "K = 5\n",
    "fewshot_indices = []\n",
    "for cls in novel_classes:\n",
    "    cls_idxs = [i for i in novel_indices if cls in ds[i][\"label\"]]\n",
    "    fewshot_indices.extend(random.sample(cls_idxs, min(K, len(cls_idxs))))\n",
    "fewshot_ds = Subset(ds, fewshot_indices)\n",
    "\n",
    "# ----- Step 3: Model (toy Faster R-CNN) -----\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"DEFAULT\"\n",
    ")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, 11  # 10 classes + background\n",
    ")\n",
    "\n",
    "# ----- Step 4: Memory Bank for MCL -----\n",
    "class MemoryBank:\n",
    "    def __init__(self, size=8192, feat_dim=256, device=\"cpu\"):\n",
    "        self.size = size\n",
    "        self.ptr = 0\n",
    "        self.feats = torch.zeros(size, feat_dim, device=device)\n",
    "        self.labels = torch.zeros(size, dtype=torch.long, device=device)\n",
    "\n",
    "    def update(self, feats, labels):\n",
    "        bsz = labels.shape[0]   # <-- luôn sync với labels\n",
    "        if bsz > self.size:\n",
    "            feats, labels = feats[:self.size], labels[:self.size]\n",
    "            bsz = self.size\n",
    "\n",
    "        idxs = (self.ptr + torch.arange(bsz)) % self.size\n",
    "        idxs = idxs.long().to(self.feats.device)\n",
    "\n",
    "        # Debug\n",
    "        # print(\"update:\", bsz, feats.shape, labels.shape, idxs.shape)\n",
    "\n",
    "        self.feats[idxs] = feats.detach()\n",
    "        # self.labels[idxs] = labels.detach()\n",
    "        self.labels[idxs] = labels.detach().view(-1)[:len(idxs)]\n",
    "\n",
    "        self.ptr = (self.ptr + bsz) % self.size\n",
    "\n",
    "    def get(self):\n",
    "        return self.feats, self.labels\n",
    "\n",
    "def supervised_contrastive_loss(features, labels, memory_feats, memory_labels, temperature=0.1):\n",
    "    # Normalize\n",
    "    features = F.normalize(features, dim=1)\n",
    "    memory_feats = F.normalize(memory_feats, dim=1)\n",
    "\n",
    "    logits = torch.mm(features, memory_feats.t()) / temperature\n",
    "    labels = labels.view(-1, 1)\n",
    "    mask = (labels == memory_labels.view(1, -1)).float()\n",
    "\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)\n",
    "    loss = -(mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
    "    return loss.mean()\n",
    "\n",
    "# ----- Step 5: Training Loop (pseudo) -----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "memory = MemoryBank(size=1024, feat_dim=in_features)\n",
    "\n",
    "# def extract_roi_features(model, images, targets):\n",
    "#     # Toy: use box_head from FasterRCNN to get features\n",
    "#     features = model.backbone(images.tensors)\n",
    "#     return features  # placeholder (need integration with roi_heads)\n",
    "\n",
    "def extract_gt_features(model, images, targets):\n",
    "    # Transform\n",
    "    transformed = model.transform(images, targets)\n",
    "    images_t, targets_t = transformed\n",
    "\n",
    "    # Backbone\n",
    "    features = model.backbone(images_t.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = {\"0\": features}\n",
    "\n",
    "    # Lấy gt boxes làm proposal\n",
    "    proposals = [t[\"boxes\"] for t in targets_t]\n",
    "\n",
    "    # ROI Pooling trên GT boxes\n",
    "    box_features = model.roi_heads.box_roi_pool(\n",
    "        features, proposals, images_t.image_sizes\n",
    "    )\n",
    "    # Head\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    labels = torch.cat([t[\"labels\"] for t in targets_t])\n",
    "    return box_features, labels\n",
    "\n",
    "for epoch in range(2):  # demo only\n",
    "    for batch in DataLoader(fewshot_ds, batch_size=2, shuffle=True, collate_fn=lambda x: x):\n",
    "        images = [s[\"image\"] for s in batch]\n",
    "        targets = []\n",
    "        for s in batch:\n",
    "            boxes = s[\"bbox_xyxy\"]\n",
    "            labels = s[\"label\"]\n",
    "            targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        detection_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # MCL: giả sử có roi_feats (cần implement thêm)\n",
    "        # roi_feats = torch.randn(len(targets), in_features, device=device)  # placeholder\n",
    "        # roi_labels = torch.cat([t[\"labels\"] for t in targets])\n",
    "\n",
    "        roi_feats, roi_labels = extract_gt_features(model, images, targets)\n",
    "\n",
    "        memory_feats, memory_labels = memory.get()\n",
    "        memory_feats, memory_labels = memory_feats.to(device), memory_labels.to(device)\n",
    "\n",
    "        if memory_labels.sum() > 0:\n",
    "            mcl_loss = supervised_contrastive_loss(roi_feats, roi_labels, memory_feats, memory_labels)\n",
    "        else:\n",
    "            mcl_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = detection_loss + 0.3 * mcl_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        memory.update(roi_feats.cpu(), roi_labels.cpu())\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ef1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to faster_rcnn_mcl_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "# Assuming the training loop is complete\n",
    "# Path to save the model\n",
    "model_path = \"faster_rcnn_mcl_finetuned.pth\"\n",
    "\n",
    "# Save the state dictionary of the model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa2939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Instantiate the same model architecture\n",
    "# You need the same model definition as before\n",
    "reloaded_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=None  # Start with no pre-trained weights\n",
    ")\n",
    "in_features = reloaded_model.roi_heads.box_predictor.cls_score.in_features\n",
    "reloaded_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, 11  # Match the output classes\n",
    ")\n",
    "\n",
    "# Step 2: Load the saved state dictionary\n",
    "model_path = \"faster_rcnn_mcl_finetuned.pth\"\n",
    "reloaded_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Step 3: Set the model to evaluation mode\n",
    "reloaded_model.eval()\n",
    "\n",
    "# Step 4: Move the model to the desired device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "reloaded_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully and ready for inference!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
